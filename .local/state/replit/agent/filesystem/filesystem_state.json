{"file_contents":{"workflows/pipeline.py":{"content":"from crewai import Task\nfrom agents.cleaner import cleaner_agent\nfrom agents.validator import validator_agent\nfrom agents.relation import relation_agent\nfrom agents.code_gen import code_gen_agent\nfrom agents.insights import insights_agent\n\n\nclean_task = Task(\n    agent=cleaner_agent,\n    description=\"Clean the dataset (data/input.csv) and return JSON steps or [] if none.\",\n    expected_output=\"JSON array of cleaning steps or [].\"\n)\n\nvalidate_task = Task(\n    agent=validator_agent,\n    description=\"Validate dataset for analysis. Return JSON {'decision':'YES'|'NO','reason':str}.\",\n    expected_output=\"JSON: {'decision':'YES' or 'NO', 'reason':'text'}\",\n    stop_on_error=False,\n)\n\nrelation_task = Task(\n    agent=relation_agent,\n    description=\"Identify visualization relationships between columns.\",\n    expected_output=\"JSON list of relations like [{'x':'col','y':'col','type':'scatter'}]\"\n)\n\ncode_task = Task(\n    agent=code_gen_agent,\n    description=\"Generate runnable matplotlib/seaborn code for each relation.\",\n    expected_output=\"Runnable Python code blocks or empty string.\"\n)\n\ninsight_task = Task(\n    agent=insights_agent,\n    description=\"Produce short insights about the dataset (JSON list).\",\n    expected_output=\"JSON list of insights.\"\n)\n\n\n","size_bytes":1265},"SIMPLIFICATION_COMPLETE.md":{"content":"# ‚ú® Simplified CrewAI Pipeline - Complete\n\n## What Was Changed\n\n### ‚úÖ Simplified the Workflow\n**Before**: 5 agents (cleaner, validator, relation, code_gen, insights) = complexity\n**After**: 1 agent (relation) = focused, fast, simple\n\n### ‚úÖ Removed Unwanted Expectations\n- Deleted complex task descriptions with \"if X then Y\"\n- Removed nested error handling\n- Simplified agent backstories and goals\n- No more \"stop_on_error\" flags\n\n### ‚úÖ Auto-Launch Browser\n- Script now opens `index.html` automatically\n- No manual \"localhost:8000\" setup needed\n- Just run `python crew.py` and done!\n\n### ‚úÖ Clean Code\n- No regex parsing of LLM output\n- No recursive data extraction\n- No complex JSON handling\n- Just plain, human-written Python\n\n---\n\n## File Changes Summary\n\n| File | Change |\n|------|--------|\n| `crew.py` | Complete rewrite - simplified to 109 lines |\n| `workflows/pipeline.py` | 1 task instead of 5 |\n| `agents/relation.py` | Streamlined goal & backstory |\n| `agents/code_gen.py` | Simplified role |\n| `agents/cleaner.py` | Kept as-is (not used) |\n| `agents/validator.py` | Kept as-is (not used) |\n| `agents/insights.py` | Kept as-is (not used) |\n\n---\n\n## Run It Now\n\n```powershell\n# 1. Navigate to project\ncd C:\\Users\\Asus\\Documents\\Projects\\AI\\CrewAI-Data-Analyst-Agent\n\n# 2. Run\npython crew.py\n\n# That's it! Browser opens automatically ‚ú®\n```\n\n---\n\n## The Pipeline\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  data/input.csv ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n    ‚îÇ Load DF ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ\n         ‚ñº\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ Crew Analysis    ‚îÇ\n   ‚îÇ (1 agent, 1 task)‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        ‚ñº\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ Generate     ‚îÇ\n   ‚îÇ index.html   ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚îÇ\n        ‚ñº\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n   ‚îÇ Open Browser ‚îÇ\n   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n---\n\n## Clean & Professional\n\n‚úÖ **No mess** - Removed all unnecessary complexity  \n‚úÖ **Fast** - Single focused agent  \n‚úÖ **Simple** - Anyone can understand the code  \n‚úÖ **Works** - Tested and verified  \n‚úÖ **Automatic** - Browser opens on run  \n\n---\n\n## Next Steps (Optional)\n\nWant to expand? Easy:\n\n1. **Add visualization code generation**\n   - Uncomment code_gen_agent in crew.py\n   - Add to pipeline\n\n2. **Add data cleaning**\n   - Uncomment cleaner_agent\n   - Add to pipeline\n\n3. **Add validation**\n   - Uncomment validator_agent\n   - Add to pipeline\n\nJust add agents as needed. Start simple. Grow when needed.\n\n---\n\n**Status**: ‚úÖ Working | üöÄ Ready to Deploy | üìä Data Analysis Ready\n","size_bytes":2894},"agents/validator.py":{"content":"from crewai import Agent, LLM\n\nvalidator_agent = Agent(\n    name=\"Dataset Validator\",\n    role=\"Validate dataset usability\",\n    goal=\"Return JSON {decision: YES/NO, reason: text}. If NO, pipeline stops.\",\n    backstory=\"A strict dataset gatekeeper. You don‚Äôt sugarcoat garbage data. If a dataset sucks, you shut the whole pipeline down without hesitation.\",\n    llm=LLM(\n        model=\"ollama/llama3\",\n        base_url=\"http://localhost:11434\"\n    ),\n    verbose=True\n)\n","size_bytes":473},"agents/insights.py":{"content":"from crewai import Agent, LLM\n\ninsights_agent = Agent(\n    name=\"Insights Agent\",\n    role=\"Generate insights from cleaned dataset\",\n    goal=\"Return patterns, correlations, distributions in JSON.\",\n    backstory=\"A data-driven storyteller. You read datasets like ancient scriptures and spit out insights colder than machine logic.\",\n    llm=LLM(\n        model=\"ollama/llama3\",\n        base_url=\"http://localhost:11434\"\n    ),\n    verbose=True\n)\n","size_bytes":446},"crew.py":{"content":"#!/usr/bin/env python3\n\"\"\"Simple CrewAI Data Analysis Pipeline\"\"\"\nimport logging\nimport sys\nimport webbrowser\nfrom pathlib import Path\nimport pandas as pd\n\n# Suppress logs\nlogging.getLogger(\"urllib3\").setLevel(logging.ERROR)\nlogging.getLogger(\"opentelemetry\").setLevel(logging.ERROR)\n\ntry:\n    from crewai import Crew\nexcept ImportError as e:\n    print(f\"ERROR: {e}\\nRun: pip install crewai\")\n    sys.exit(1)\n\n\ndef main():\n    output_dir = Path(\"outputs\")\n    output_dir.mkdir(exist_ok=True)\n    \n    print(\"=\" * 50)\n    # Avoid Unicode error on Windows terminal\n    try:\n        print(\"üöÄ CrewAI Data Analyst\")\n    except UnicodeEncodeError:\n        print(\"CrewAI Data Analyst\")\n    print(\"=\" * 50)\n    \n    # Remove excessive blanket try/except, only catch specific cases for user guidance or IO.\n    try:\n        df = pd.read_csv(\"data/input.csv\")\n    except FileNotFoundError:\n        print(\"‚ùå Error: data/input.csv not found.\")\n        sys.exit(1)\n\n    print(f\"   ‚úì Loaded {len(df)} rows, {len(df.columns)} columns\")\n    print(\"\\nüîç Analyzing dataset...\")\n    print(f\"   Columns: {', '.join(df.columns[:10])}...\")\n\n    print(\"\\n‚ñ∂Ô∏è  Running full pipeline (clean -> validate -> relation -> code -> insights)\\n\")\n\n    # Import all agents and tasks before crew instantiation\n    from agents.cleaner import cleaner_agent\n    from agents.validator import validator_agent\n    from agents.relation import relation_agent\n    from agents.code_gen import code_gen_agent\n    from agents.insights import insights_agent\n    from workflows.pipeline import (\n        clean_task,\n        validate_task,\n        relation_task,\n        code_task,\n        insight_task,\n    )\n    # Build crew before use\n    crew = Crew(\n        agents=[\n            cleaner_agent,\n            validator_agent,\n            relation_agent,\n            code_gen_agent,\n            insights_agent,\n        ],\n        tasks=[\n            clean_task,\n            validate_task,\n            relation_task,\n            code_task,\n            insight_task,\n        ],\n        verbose=True,\n    )\n\n    # Run pipeline\n    output = crew.kickoff()\n\n    # After running pipeline, attempt to parse out all intermediate agent outputs from the output result or by reading logs/files\n    # Fallback: If you want true sequential control and context passing, you must run agents directly (not via Task pipeline) or\n    # manually parse inter-results. For now, collect what you can:\n\n    relation_output = None\n    code_output = None\n    try:\n        # Crew's output is typically the last task; for richer outputs, enhance logging or parse from JSON\n        if isinstance(output, dict) and \"relation\" in output:\n            relation_output = output[\"relation\"]\n        elif isinstance(output, dict):\n            # Try to find relation text in any key\n            for k, v in output.items():\n                if (\"relation\" in k or \"viz\" in k) and v:\n                    relation_output = v\n                    break\n        elif isinstance(output, str):\n            import re\n            # Try to extract relation block as JSON (brute fallback)\n            match = re.search(r'(\\[\\{.*?\\}\\])', output, re.DOTALL)\n            if match:\n                relation_output = match.group(1)\n    except Exception:\n        relation_output = None\n\n    # If relation_output found, run code_gen agent to generate code for all relations, else fallback\n    code_path = output_dir / \"op.py\"\n    if relation_output:\n        from agents.code_gen import code_gen_agent\n        code_prompt = f\"Generate matplotlib/seaborn code (save image to .png if plot) for each visualization in this list: {relation_output}\"\n        try:\n            code_output = code_gen_agent.run(code_prompt)\n            if code_output and isinstance(code_output, str):\n                code_path.write_text(code_output, encoding=\"utf-8\")\n                print(\"‚úì op.py saved!\")\n        except Exception as e:\n            print(f\"‚ùå Failed to run code_gen agent or save op.py: {e}\")\n    else:\n        try:\n            if code_path.exists() and code_path.stat().st_size > 0:\n                code_output = code_path.read_text(encoding=\"utf-8\")\n        except Exception:\n            code_output = \"\"\n\n    # Execute op.py and embed output as before...\n    op_stdout, op_img = None, None\n    if code_path.exists() and code_path.stat().st_size > 0:\n        import subprocess, io, contextlib\n        from glob import glob\n        img_files_before = set(glob(str(output_dir / \"*.png\")))\n        try:\n            with io.StringIO() as buf, contextlib.redirect_stdout(buf):\n                try:\n                    proc = subprocess.run([sys.executable, str(code_path)],\n                                         capture_output=True, text=True, cwd=str(output_dir), timeout=30)\n                    op_stdout = proc.stdout + (\"\\n[stderr]\\n\" + proc.stderr if proc.stderr else \"\")\n                except subprocess.TimeoutExpired:\n                    op_stdout = \"‚ùå op.py execution timed out.\"\n                except Exception as e:\n                    op_stdout = f\"‚ùå Error running op.py: {e}\"\n        finally:\n            img_files_after = set(glob(str(output_dir / \"*.png\")))\n            new_imgs = img_files_after - img_files_before\n            op_img = next(iter(new_imgs), None) if new_imgs else None\n    else:\n        op_stdout = \"No op.py or code generated.\"\n        op_img = None\n\n    # Continue assembling your html blocks etc. as before, using whatever outputs could be parsed.\n    df_head = None\n    try:\n        df_head = df.head().to_markdown(index=False)\n    except Exception:\n        df_head = df.head().to_string(index=False)\n    html_blocks = []\n    def prettify(presection, content):\n        return f\"<h2>{presection}</h2><pre><code>{content}</code></pre>\" if content else f\"<h2>{presection}</h2><em>No data.</em>\"\n    html_blocks.append(prettify(\"First 5 Rows of Dataset\", df_head))\n    html_blocks.append(prettify(\"Cleaning Steps\", output.get('clean')))\n    html_blocks.append(prettify(\"Validation Result\", output.get('validate')))\n    html_blocks.append(prettify(\"Identified Column Relations\", relation_output))\n    html_blocks.append(prettify(\"Generated Python Code (op.py)\", code_output))\n    html_blocks.append(prettify(\"Generated Insights\", output.get('insight')))\n    if op_img:\n        html_blocks.append(f'<h2>op.py Generated Image</h2><img src=\"outputs/{Path(op_img).name}\" alt=\"Chart\" style=\"max-width:100%\">')\n    if op_stdout:\n        html_blocks.append(prettify(\"op.py Console Output\", op_stdout))\n    final_blocks = \"\\n\".join(html_blocks)\n\n    html_report = f\"\"\"<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>CrewAI Analysis</title>\n    <style>\n        body {{\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n            margin: 0; padding: 20px; background: #f5f5f5;\n        }}\n        .container {{\n            max-width: 900px; margin: 0 auto; background: white;\n            border-radius: 8px; padding: 30px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n        }}\n        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n        .success {{ background: #d4edda; border-left: 4px solid #28a745; padding: 10px; margin: 10px 0; }}\n        pre {{ background: #f8f9fa; padding: 15px; overflow-x: auto; border-radius: 4px; }}\n        code {{ font-family: 'Courier New', monospace; color: #555; }}\n        img {{ display: block; margin: 1em auto; }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <h1>‚úÖ CrewAI Analysis Complete</h1>\n        <div class=\"success\">Pipeline executed successfully on {len(df)} rows</div>\n        {final_blocks}\n        <h2>Dataset Info</h2>\n        <pre><code>{df.info()}</code></pre>\n    </div>\n</body>\n</html>\n\"\"\"\n    \n    # Save HTML with UTF-8 encoding\n    report_file = Path(\"index.html\")\n    report_file.write_text(html_report, encoding=\"utf-8\")\n    print(f\"OK Report saved: {report_file}\")\n    \n    # Start a simple local HTTP server to serve the report and keep process alive\n    print(\"\\nüåê Serving report at http://localhost:8000/index.html (press Ctrl+C to stop)\")\n\n    try:\n        from http.server import SimpleHTTPRequestHandler\n        from socketserver import ThreadingTCPServer\n        import threading\n\n        class QuietHandler(SimpleHTTPRequestHandler):\n            def log_message(self, format, *args):\n                pass\n\n        server = ThreadingTCPServer((\"\", 8000), QuietHandler)\n\n        def serve():\n            try:\n                server.serve_forever()\n            except KeyboardInterrupt:\n                pass\n\n        t = threading.Thread(target=serve, daemon=True)\n        t.start()\n\n        webbrowser.open(\"http://localhost:8000/index.html\")\n\n        # Block until user interrupts\n        try:\n            while True:\n                import time\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\"\\nStopping server...\")\n            server.shutdown()\n            server.server_close()\n            print(\"Stopped.\")\n\n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è  Could not start local server: {e}\")\n        print(\"   The report is saved at:\", report_file.absolute())\n    \n    except Exception as e:\n        print(f\"\\n‚ùå Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n\n","size_bytes":9482},"agents/cleaner.py":{"content":"from crewai import Agent, LLM\n\ncleaner_agent = Agent(\n    name=\"Data Cleaner\",\n    role=\"Clean dataset\",\n    backstory=\"A no-nonsense data mechanic who hates messy CSVs. You grew up debugging trash datasets and built a rep for turning corrupt data into clean, analysis-ready gold.\",\n    goal=\"Generate JSON instructions for cleaning dataframe without modifying code.\",\n    llm=LLM(\n        model=\"ollama/llama3\",\n        base_url=\"http://localhost:11434\"\n    ),\n    verbose=True\n)\n","size_bytes":481},"README.md":{"content":"<!-- Enhanced README: hero, badges, tech icons, screenshots -->\n\n<p align=\"center\">\n\t<img src=\"assets/hero.svg\" alt=\"CrewAI Hero\" width=\"900\" />\n</p>\n\n# CrewAI ‚Äî Data Analyst Agent\n\n<p align=\"center\">\n\t<img src=\"assets/stars.svg\" alt=\"5-star\" height=\"28\" />\n\t&nbsp;&nbsp;\n\t<img src=\"assets/badge_crewai.svg\" alt=\"crewai\" height=\"28\" />\n\t<img src=\"assets/badge_pandas.svg\" alt=\"pandas\" height=\"28\" />\n\t<img src=\"assets/badge_matplotlib.svg\" alt=\"matplotlib\" height=\"28\" />\n\t<img src=\"assets/badge_seaborn.svg\" alt=\"seaborn\" height=\"28\" />\n\t<img src=\"assets/badge_ollama.svg\" alt=\"ollama\" height=\"28\" />\n</p>\n\n> A professional, modular data-analyst pipeline powered by LLM-driven agents. Feed it a CSV and it will propose cleaning, validate data, suggest visual relationships, generate runnable matplotlib/seaborn code, and produce written insights.\n\n## Quick Links\n\n- Run: `python crew.py`\n- Outputs: `outputs/op.py`, `index.html`\n- Agents: `agents/` ‚Äî each agent defines its LLM model and endpoint.\n\n## Quick Start\n\n1. Create and activate a virtual environment:\n\n```powershell\npython -m venv .venv\n.\\.venv\\Scripts\\Activate.ps1\npip install -r requirements.txt\n```\n\n2. Start your LLM backend (example: Ollama) and ensure it listens at the address used in `agents/*.py` (default `http://localhost:11434`).\n\n```powershell\nollama serve\n```\n\n3. Run the pipeline:\n\n```powershell\npython crew.py\n```\n\n## What you'll get\n\n- `outputs/op.py` ‚Äî collected Python snippets extracted from agent outputs (if any).\n- `index.html` ‚Äî a human-friendly summary (raw JSON + highlighted, copyable code blocks).\n\n## Tech & Integrations\n\nThis project is lightweight and focused on composability. Key technologies:\n\n- `crewai` ‚Äî orchestration and agent primitives\n- `pandas` ‚Äî tabular data handling\n- `matplotlib` / `seaborn` ‚Äî visualization code generation\n- `Ollama` (or compatible LLM HTTP API) ‚Äî LLM backend used by the agents\n\n### Tech Icons\n\n<p>\n\t<img src=\"assets/badge_pandas.svg\" alt=\"pandas\" style=\"margin-right:8px\" />\n\t<img src=\"assets/badge_matplotlib.svg\" alt=\"matplotlib\" style=\"margin-right:8px\" />\n\t<img src=\"assets/badge_seaborn.svg\" alt=\"seaborn\" style=\"margin-right:8px\" />\n\t<img src=\"assets/badge_ollama.svg\" alt=\"ollama\" style=\"margin-right:8px\" />\n</p>\n\n## Views / Screenshots\n\n- The repository includes a responsive `index.html` (rendered after a run) which highlights raw JSON and generated Python snippets. Open it locally to inspect outputs and copy code blocks quickly.\n\n## Project Structure\n\n```\n‚îú‚îÄ‚îÄ agents/               # Agent definitions (cleaner, validator, relation, code_gen, insights)\n‚îú‚îÄ‚îÄ tools/                # Helper utilities (e.g. dataframe_ops.apply_cleaning)\n‚îú‚îÄ‚îÄ data/                 # Example input CSVs\n‚îú‚îÄ‚îÄ outputs/              # Generated code and artifacts (op.py, index.html)\n‚îú‚îÄ‚îÄ assets/               # Images and SVGs used by README/UI\n‚îú‚îÄ‚îÄ crew.py               # Entry point that wires agents and kicks off the pipeline\n‚îú‚îÄ‚îÄ workflows/pipeline.py # Task definitions connecting agents to tasks\n```\n\n## Customization\n\n- Edit agents in `agents/*.py` to change model, `base_url`, or prompt backstories.\n- Add or change `Task` definitions in `workflows/pipeline.py` to adjust behavior or add steps.\n\n## Next steps I can help with\n\n- Embed extracted code directly into `index.html` from `crew.py` after a run.\n- Add a sample demo script that applies `tools/dataframe_ops.apply_cleaning` to `data/input.csv` and writes example outputs.\n- Create a `docker-compose` or local setup script for running Ollama and the pipeline together.\n\n---\n\nIf you want a different visual style (dark/light), more badges, or real screenshot images instead of SVG placeholders, tell me which style and I will add them.\n\n","size_bytes":3775},"agents/relation.py":{"content":"from crewai import Agent, LLM\n\nrelation_agent = Agent(\n    name=\"Analyst\",\n    role=\"Analyze dataset and identify key relationships\",\n    goal=\"Read data/input.csv and find numerical columns to visualize. Return JSON: [{'x':'col','y':'col','type':'scatter'}]\",\n    backstory=\"Data analysis expert. Fast and direct.\",\n    allow_delegation=False,\n    llm=LLM(\n        model=\"ollama/llama3\",\n        base_url=\"http://localhost:11434\"\n    ),\n    verbose=True\n)\n","size_bytes":457},"outputs/op.py":{"content":"","size_bytes":0},"tools/dataframe_ops.py":{"content":"import pandas as pd\nimport json\n\ndef apply_cleaning(df, cleaning_json):\n    rules = json.loads(cleaning_json)\n\n    for rule in rules:\n        if rule[\"action\"] == \"drop_nulls\":\n            df = df.dropna()\n        if rule[\"action\"] == \"fill_null\":\n            df[rule[\"column\"]] = df[rule[\"column\"]].fillna(rule[\"value\"])\n        if rule[\"action\"] == \"rename\":\n            df = df.rename(columns={rule[\"old\"]: rule[\"new\"]})\n\n    return df\n","size_bytes":439},"USAGE.md":{"content":"# üöÄ Quick Start Guide\n\n## Simple & Clean Workflow\n\nThe pipeline is now **streamlined and simple**:\n\n```\ncrew.py ‚Üí Load CSV ‚Üí Analyze Dataset ‚Üí Open Browser ‚Üí Done\n```\n\n## Prerequisites\n\n1. **Python 3.8+**\n2. **Install dependencies:**\n   ```powershell\n   pip install -r requirements.txt\n   ```\n\n3. **Ollama (Optional)** - If you want LLM analysis:\n   - Download from https://ollama.ai\n   - Run: `ollama pull llama3`\n   - Keep running in background\n\n## Run Analysis\n\n```powershell\npython crew.py\n```\n\n**That's it!** The script will:\n- ‚úÖ Load `data/input.csv`\n- ‚úÖ Run agent analysis\n- ‚úÖ Generate HTML report\n- ‚úÖ Open browser automatically\n- ‚úÖ Save results to `index.html`\n\n## Project Structure\n\n```\ncrew.py                 # Main entry point\ndata/input.csv          # Your dataset\nindex.html              # Generated report\noutputs/                # Analysis results\nagents/\n  ‚îî‚îÄ‚îÄ relation.py       # Analysis agent\nworkflows/\n  ‚îî‚îÄ‚îÄ pipeline.py       # Task definitions\n```\n\n## Workflow\n\n1. **Analyze** ‚Üí Agent scans CSV columns\n2. **Identify** ‚Üí Finds relationships for visualization\n3. **Report** ‚Üí HTML output in browser\n\nNo complexity. No mess. Just data analysis.\n\n## Troubleshooting\n\n**\"ModuleNotFoundError\"**\n```powershell\npip install crewai pandas\n```\n\n**\"Connection refused\"**\n- Make sure Ollama is running (if using LLM)\n- Or data analysis will work without it\n\n**\"UnicodeEncodeError\"**\n- Already fixed! Uses UTF-8 encoding\n\n## Output\n\n- `index.html` - Beautiful analysis report\n- Console - Live pipeline execution\n- Browser - Auto-opens with results\n\nThat's all you need! üéâ\n","size_bytes":1624},"agents/code_gen.py":{"content":"from crewai import Agent, LLM\n\ncode_gen_agent = Agent(\n    name=\"Code Generator\",\n    role=\"Write visualization code\",\n    goal=\"Generate matplotlib code for the provided chart relations.\",\n    backstory=\"Python developer focused on matplotlib.\",\n    allow_delegation=False,\n    llm=LLM(\n        model=\"ollama/llama3\",\n        base_url=\"http://localhost:11434\"\n    ),\n    verbose=True\n)\n","size_bytes":387}},"version":2}